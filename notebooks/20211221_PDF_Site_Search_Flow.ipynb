{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import re\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import io\n",
    "\n",
    "from PyPDF2 import utils\n",
    "import datetime as dt\n",
    "from time import mktime, strptime\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This notebook contains a demonstration of a proposed flow of alerting Banktrack of new PDF policies.\n",
    "\n",
    "The flow goes like:\n",
    "1. Scrape banktrack bank page for all stored pdf links\n",
    "2. Use the scraped pdfs to key word google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(pdf, pages=None):\n",
    "    \"\"\"\"\"\"\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    r = requests.get(pdf)\n",
    "    f = io.BytesIO(r.content)\n",
    "\n",
    "    for page in PDFPage.get_pages(f, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def scrape_banktrack_links():\n",
    "    \"\"\"\"\"\"\n",
    "    # get the url from requests get method\n",
    "    read = requests.get(BANKTRACK_URL)\n",
    "\n",
    "    # full html content\n",
    "    html_content = read.content\n",
    "\n",
    "    # Parse the html content\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # created an empty list for putting the pdfs\n",
    "    list_of_pdf = set()\n",
    "\n",
    "    # accessed the first p tag in the html\n",
    "    l = soup.find(\"div\", {\"class\": \"policyfiles\"})\n",
    "\n",
    "    # accessed all the anchors tag from given p tag\n",
    "    p = l.find_all(href=True)\n",
    "\n",
    "    banktrack_links = []\n",
    "    for a in p:\n",
    "        banktrack_links.append(a['href'])\n",
    "        \n",
    "    return banktrack_links\n",
    "\n",
    "\n",
    "def categorise_links(links):\n",
    "    \"\"\"\"\"\"\n",
    "    dct_link_types = {'banktrack':\n",
    "                          {'pdf': [],\n",
    "                           'html': []},\n",
    "                      'bank':\n",
    "                          {'pdf': [],\n",
    "                           'html': []}}\n",
    "\n",
    "    for link in links:\n",
    "        time.sleep(.5)\n",
    "        try:\n",
    "            r = requests.get(link,  timeout=5)\n",
    "        except requests.exceptions.ConnectTimeout as e:\n",
    "            print(\"timeout\")\n",
    "            continue\n",
    "\n",
    "        domain = urllib.parse.urlparse(link).netloc\n",
    "        content_type = r.headers.get('content-type')\n",
    "\n",
    "        if 'application/pdf' in content_type:\n",
    "            if domain == 'www.banktrack.org':\n",
    "                dct_link_types['banktrack']['pdf'].append(link)\n",
    "            else:\n",
    "                dct_link_types['bank']['pdf'].append(link)\n",
    "\n",
    "        elif 'text/html' in content_type:\n",
    "            if domain == 'www.banktrack.org':\n",
    "                dct_link_types['banktrack']['html'].append(link)\n",
    "            else:\n",
    "                dct_link_types['bank']['html'].append(link)\n",
    "        else:\n",
    "            ext = ''\n",
    "            print('Unknown type: {}'.format(content_type))\n",
    "            \n",
    "    return dct_link_types\n",
    "\n",
    "\n",
    "def get_pdf_meta(lst_pdf_links):\n",
    "    \"\"\"\"\"\"\n",
    "    dct_pdf_meta = {}\n",
    "\n",
    "    for pdf in lst_pdf_links:\n",
    "        try:\n",
    "            txt = convert_pdf_to_txt(pdf, pages=[0])\n",
    "        except utils.PdfReadError as e:\n",
    "            print(f\"can't read {pdf}\")\n",
    "\n",
    "        r = requests.get(pdf)\n",
    "        f = io.BytesIO(r.content)\n",
    "\n",
    "        parser = PDFParser(f)\n",
    "        doc = PDFDocument(parser)\n",
    "        creation_date = doc.info[0][\"CreationDate\"].decode(\"utf-8\")\n",
    "\n",
    "        creation_date = creation_date.rstrip(\"Z\").split(\"+\")[0].split(\"-\")[\n",
    "            0].rstrip(\"Z'\")\n",
    "        creation_date = str(dt.datetime.fromtimestamp(mktime(strptime(\n",
    "            creation_date[2:], \"%Y%m%d%H%M%S\"))))\n",
    "\n",
    "        sample_txt = txt.replace(\"\\n\", '').strip(\" \")[0:50]\n",
    "\n",
    "        dct_pdf_meta[sample_txt] = {}\n",
    "        dct_pdf_meta[sample_txt]['url'] = pdf\n",
    "        dct_pdf_meta[sample_txt]['created_at'] = creation_date\n",
    "\n",
    "    return dct_pdf_meta\n",
    "\n",
    "def scrape_search_links(bank_track_links):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    all_search_pdf_links = []\n",
    "\n",
    "    for pdf in bank_track_links:\n",
    "\n",
    "        pdf_name = pdf.split(\"/\")[-1]\n",
    "        pdf_name = re.sub(r'\\d+', '', pdf_name)\n",
    "        pdf_name = pdf_name.replace(\"_\", \" \")\n",
    "\n",
    "        query = f\"site:{BANK_DOMAIN} {pdf_name}\"\n",
    "        print(f\"searching for {query}\")\n",
    "\n",
    "        # to search\n",
    "        search_results = search(query, tld=\"co.in\", num=3, stop=3,\n",
    "                                pause=2)\n",
    "\n",
    "        pdf_search_links = list(filter(lambda x: x.endswith('.pdf'),\n",
    "                                       search_results))\n",
    "\n",
    "        all_search_pdf_links.extend(pdf_search_links)\n",
    "        \n",
    "    return all_search_pdf_links\n",
    "\n",
    "\n",
    "def get_new_pdfs(banktrack_pdf_meta, search_pdf_meta):\n",
    "\n",
    "    doc_ints = set(search_pdf_meta.keys()).\\\n",
    "        intersection(set(banktrack_pdf_meta.keys()))\n",
    "\n",
    "    print(f\"There are {len(banktrack_pdf_meta)} banktrack pdfs\")\n",
    "    print(f\"There are {len(search_pdf_meta)} pdfs found from search\")\n",
    "    print(f\"The intersection of these is {len(doc_ints)} pdfs\")\n",
    "\n",
    "    non_matching_pdfs = set(search_pdf_meta.keys()) - set(\n",
    "        banktrack_pdf_meta.keys())\n",
    "\n",
    "    dct_non_matching_pdfs = {k: search_pdf_meta[k] for k in\n",
    "                            non_matching_pdfs}\n",
    "\n",
    "    sorted_by_date = reversed(sorted(dct_non_matching_pdfs, key=lambda x:\n",
    "    dct_non_matching_pdfs[x][\n",
    "        'created_at']))\n",
    "\n",
    "    dct_non_matching_pdfs = {k: search_pdf_meta[k] for k in\n",
    "                             sorted_by_date}\n",
    "    \n",
    "    return dct_non_matching_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANKTRACK_URL = \"https://www.banktrack.org/bank/barclays#policies\"\n",
    "BANK_DOMAIN = 'home.barclays'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all policy links from the banktrack website\n",
    "bt_links = scrape_banktrack_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeout\n"
     ]
    }
   ],
   "source": [
    "# categorise links into 4 categories:\n",
    "# 1. PDFs stored on Banktrack\n",
    "# 2. PDFs stored on Bank's website\n",
    "# 3. HTML links to Banktrack\n",
    "# 4. HTML links to Banktrack\n",
    "dct_bt_links = categorise_links(bt_links)\n",
    "\n",
    "# collect all pdf links\n",
    "banktrack_pdf_links = (dct_bt_links['banktrack']['pdf'] + \n",
    "                       dct_bt_links['bank']['pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped links for 30 pdfs\n"
     ]
    }
   ],
   "source": [
    "print(f\"Scraped links for {len(banktrack_pdf_links)} pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the keywords found in each scraped link as a search term \n",
    "# and collect the top 3 pdfs from each search\n",
    "search_pdf_links = scrape_search_links(banktrack_pdf_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pdf metadata for the pdf from banktrack policy page and from the google site searches \n",
    "banktrack_pdf_meta = get_pdf_meta(banktrack_pdf_links)\n",
    "search_pdf_meta = get_pdf_meta(search_pdf_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30 banktrack pdfs\n",
      "There are 0 pdfs found from search\n",
      "The intersection of these is 0 pdfs\n"
     ]
    }
   ],
   "source": [
    "# get the pdfs that were found in search but are not on banktrack\n",
    "dct_new_pdfs = get_new_pdfs(banktrack_pdf_meta, search_pdf_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIBC Code of ConductNovember 2021CIBC External App - Created on: 2021-10-22 15:26:27; URL - https://www.cibc.com/content/dam/about_cibc/corporate_governance/pdfs/code-of-conduct-en.pdf\n",
      "RESPONSIBLE INVESTING POLICYCIBC Asset Management  - Created on: 2021-08-09 16:50:02; URL - https://www.cibc.com/content/dam/cam-public-assets/documents/cibc-cam-our-approach-responsible-investment-en.pdf\n",
      "Sustainability Report 2020 MENUMENU 1.0 Overview1. - Created on: 2021-03-19 10:17:48; URL - https://www.cibc.com/content/dam/about_cibc/corporate_responsibility/pdfs/cibc-esg-2020-en.pdf\n",
      "Canadian Imperial Bank of Commerce (CIBC) - Climat - Created on: 2020-08-27 14:52:47; URL - https://www.cibc.com/content/dam/cibc-public-assets/about-cibc/corporate-responsibility/environment/documents/cibc-cdp-climate-change-response-2020-en.pdf\n",
      "CIBC Supplier Code of ConductPurpose Our vision is - Created on: 2020-08-14 10:00:38; URL - https://www.cibc.com/ca/pdf/about/supplier-code-of-conduct-en.pdf\n",
      "MODERN SLAVERY ACT STATEMENT This statement has be - Created on: 2019-08-27 16:15:48; URL - https://www.cibc.com/content/dam/about_cibc/corporate_responsibility/pdfs/modern-slavery-act-statement-2019-en.pdf\n",
      "CIBC CODE OF CONDUCT   INTRODUCTION  The Canadian  - Created on: 2007-06-20 16:40:06; URL - https://www.cibc.com/ca/pdf/about/code-cndct-03.pdf\n",
      "CIBC 1034 E_16-36_v2  12/8/05  9:13 PM  Page 3030C - Created on: 2005-12-09 12:09:06; URL - https://www.cibc.com/ca/pdf/about/pas-environment-05-en.pdf\n"
     ]
    }
   ],
   "source": [
    "for txt, meta in dct_new_pdfs.items():\n",
    "\n",
    "    print(f\"{txt} - Created on: {meta['created_at']}; \"\n",
    "          f\"URL - {meta['url']}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt, meta in dct_new_pdfs.items():\n",
    "\n",
    "    print(f\"{txt} - Created on: {meta['created_at']}; \"\n",
    "          f\"URL - {meta['url']}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
