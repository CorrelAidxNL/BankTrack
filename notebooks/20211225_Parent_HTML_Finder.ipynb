{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PyPDF2 import utils\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser, PDFSyntaxError\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from requests.exceptions import MissingSchema\n",
    "\n",
    "import datetime as dt\n",
    "from time import mktime, strptime\n",
    "\n",
    "from io import StringIO\n",
    "import io\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BankTrackDataProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # get the json\n",
    "        self.bt_dict = dict(requests.get(BT_JSON_URL).json())\n",
    "\n",
    "    def get_bank_links(self) -> pd.DataFrame:\n",
    "        \"\"\"\"\"\"\n",
    "        # get bank links\n",
    "        df_bank_links = pd.DataFrame.from_dict(self.bt_dict['csrdocs'])\n",
    "        \n",
    "        # get external link if available, otherwise internal\n",
    "        df_bank_links['link'] = np.where(df_bank_links['link_external'] == '', \n",
    "                                         df_bank_links['link_internal'],\n",
    "                                         df_bank_links['link_external'])\n",
    "        \n",
    "        df_bank_links = df_bank_links.explode('bp_ids')\n",
    "        return df_bank_links\n",
    "\n",
    "    def get_bank_id(self) -> int:\n",
    "        \"\"\"\"\"\"\n",
    "        # get bank meta\n",
    "        bank_meta = pd.DataFrame.from_dict(self.bt_dict['bps'], orient='index')\n",
    "        bank_id = bank_meta.query(\"title == @BANK_NAME\").id.squeeze()\n",
    "        return bank_id\n",
    "\n",
    "\n",
    "def get_match_score(string: str) -> int:\n",
    "    \"\"\"\n",
    "    returns the number of policy keywords contained\n",
    "    by the input string\n",
    "    \"\"\"\n",
    "\n",
    "    match_score = 0\n",
    "    string = string.lower()\n",
    "    for kw in POLICY_KEYWORDS:\n",
    "        match_score += kw in string\n",
    "        \n",
    "    return match_score\n",
    "\n",
    "def convert_pdf_to_txt(pdf_link: str, pages=None) -> str:\n",
    "    \"\"\"Converts pdf to text\"\"\"\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    r = requests.get(pdf_link)\n",
    "    f = io.BytesIO(r.content)\n",
    "\n",
    "    for page in PDFPage.get_pages(f, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_pdf_meta(pdf_link: str) -> Tuple[str, str]:\n",
    "    \"\"\"Given a pdf link, returns the first 50 characters and\n",
    "    the creation date, if available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        txt = convert_pdf_to_txt(pdf_link, pages=[0])\n",
    "    except (utils.PdfReadError, PDFSyntaxError) as e:\n",
    "        print(f\"can't read {pdf_link}\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    r = requests.get(pdf_link)\n",
    "    f = io.BytesIO(r.content)\n",
    "\n",
    "    parser = PDFParser(f)\n",
    "    doc = PDFDocument(parser)\n",
    "    try:\n",
    "        creation_date = doc.info[0][\"CreationDate\"].decode(\"utf-8\")\n",
    "\n",
    "        creation_date = creation_date.rstrip(\"Z\").split(\"+\")[0].split(\"-\")[\n",
    "            0].rstrip(\"Z'\")\n",
    "        creation_date = str(dt.datetime.fromtimestamp(mktime(strptime(\n",
    "            creation_date[2:], \"%Y%m%d%H%M%S\"))))\n",
    "    except KeyError:\n",
    "        print(f\"Can't fetch date for PDF {pdf_link}\")\n",
    "        creation_date = 'NA'\n",
    "\n",
    "    sample_txt = txt.replace(\"\\n\", '').strip(\" \")[0:50]\n",
    "    if sample_txt == '\\x0c':\n",
    "        print(f\"Couldn't extract text from {pdf_link}. Likely the PDF is \"\n",
    "              f\"protected against copy\")\n",
    "\n",
    "    return sample_txt, creation_date\n",
    "\n",
    "\n",
    "def garnish_with_pdf_meta(link_df_in: pd.DataFrame, \n",
    "                          url_field: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds pdf metadata to each pdf link in a dataframe.\n",
    "    'sample_text' and 'created_at' fields are added\n",
    "    \"\"\"\n",
    "    link_df = link_df_in.copy()\n",
    "    \n",
    "    # get pdf metadata\n",
    "    meta_tpl = link_df[url_field].map(get_pdf_meta)\n",
    "    \n",
    "    # add metadata to the dataframe\n",
    "    link_df['sample_text'] = [tpl[0] for tpl in meta_tpl]\n",
    "    link_df['created_at'] = [tpl[1] for tpl in meta_tpl]\n",
    "\n",
    "    na_mask = link_df['sample_text'].isna()\n",
    "    if na_mask.any():\n",
    "        print(f\"{na_mask.sum()} missing values in sample text field. \"\n",
    "              f\"Dropping these rows\")\n",
    "        link_df.dropna(subset=['sample_text'], inplace=True)\n",
    "    \n",
    "    return link_df\n",
    "\n",
    "def process_scraped_links(dct_html_to_links) -> pd.DataFrame:\n",
    "\n",
    "    # collect all the scraped htmls and pdfs in a dataframe\n",
    "    df_scraped_links = pd.DataFrame.from_dict(dct_html_to_links, orient='index')\n",
    "\n",
    "    # process the scraped links dataframe\n",
    "    df_scraped_links['scraped_url'] = df_scraped_links['scraped_url'].map(list)\n",
    "\n",
    "    df_scraped_links = (\n",
    "        df_scraped_links.\n",
    "        explode('scraped_url').\n",
    "        pipe(lambda df: df.assign(\n",
    "            filename = df['scraped_url'].str.split('/').str[-1]\n",
    "        )\n",
    "            ).\n",
    "        reset_index().\n",
    "        rename({'index': 'parent_html'}, axis=1)\n",
    "    )\n",
    "\n",
    "    return df_scraped_links\n",
    "\n",
    "def collect_parent_htmls_per_file(df_scraped_links: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # collect all parent htmls per file\n",
    "    df_parent_htmls = (\n",
    "        df_scraped_links.\n",
    "        pipe(lambda df: df.assign(\n",
    "            parent_html = df['parent_html'].str.replace('https:/', 'https://').str.replace('///', '//'),\n",
    "            scraped_url = df['scraped_url'].str.replace('https:/', 'https://').str.replace('///', '//'))).\n",
    "        groupby(\"filename\").\n",
    "        agg(parent_htmls=('parent_html', list)).\n",
    "        reset_index()\n",
    "    )\n",
    "\n",
    "    # merge scraped links back to collected parent htmls using filename \n",
    "    df_scraped_links = (\n",
    "        df_parent_htmls.\n",
    "        merge(df_scraped_links.\n",
    "              drop_duplicates('filename'), \n",
    "              on='filename', how='left').\n",
    "        drop('parent_html', axis=1)\n",
    "    )\n",
    "    \n",
    "    return df_scraped_links\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Description\n",
    "\n",
    "This notebook provides prototype code for scraping through the bank's website and retrieving parent HTMLs of PDFs stored by BankTrack.\n",
    "\n",
    "The script also retrieves any other HTMLs or PDFs that it thinks may be relevant based on a list of keywords.\n",
    "\n",
    "The scraper is designed likes this:\n",
    "1. Starts on some link (such as the ESG url linked by BankTrack). \n",
    "2. Collects all links from that page. \n",
    "3. If there are any PDFs that match keywords during step 2, then we save these and the parent HTML\n",
    "4. The links are sorted, to put the ones that match `POLICY_KEYWORDS` at the front. \n",
    "5. Most relevant link is accessed\n",
    "\n",
    "Steps 2-5 are repeated until all links were visited or we have visited a more than N links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BT_JSON_URL = 'https://www.banktrack.org/service/sections/Document/csrdata'\n",
    "\n",
    "POLICY_KEYWORDS = ('climate', 'esg', 'slave', 'palm', 'oil', 'coal', \n",
    "                   'forest', 'defence', 'environment', 'energy', 'bribery', 'corrupt', \n",
    "                   'human', 'rights', 'mine', 'mining', 'metal', 'power', 'waste','impact',\n",
    "                   'mountain', 'sustain', 'fish', 'agricult', 'commodit', 'conduct')\n",
    "\n",
    "BANK_NAME = 'Citi'\n",
    "BASE_URL = \"https://www.citigroup.com/\" # must be the bare minimum needed to acceess the bank website\n",
    "START_URLS = {'https://www.citigroup.com/citi/sustainability/policies.htm'} # should be the ESG link(s) linked by banktrack\n",
    "\n",
    "bank_domain = BASE_URL.split('//')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get BankTrack stored links for the bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tag</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link_external</th>\n",
       "      <th>link_internal</th>\n",
       "      <th>file</th>\n",
       "      <th>pubpriv</th>\n",
       "      <th>bp_ids</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>10979</td>\n",
       "      <td>16af3ec</td>\n",
       "      <td>2014-01-01 00:00:00</td>\n",
       "      <td>Sector briefs - Thermal power</td>\n",
       "      <td></td>\n",
       "      <td>https://www.banktrack.org/download/16af3ec</td>\n",
       "      <td>1160856_sector_brief_thermal_power.pdf</td>\n",
       "      <td>pub</td>\n",
       "      <td>50</td>\n",
       "      <td>https://www.banktrack.org/download/16af3ec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>11279</td>\n",
       "      <td>forestry_pdf</td>\n",
       "      <td>2013-12-31 00:00:00</td>\n",
       "      <td>Sustainable Forestry Standard</td>\n",
       "      <td></td>\n",
       "      <td>https://www.banktrack.org/download/forestry_pdf</td>\n",
       "      <td>forestry.pdf</td>\n",
       "      <td>pub</td>\n",
       "      <td>50</td>\n",
       "      <td>https://www.banktrack.org/download/forestry_pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>11677</td>\n",
       "      <td>sustainable_progress_citi_s_five_year_sustaina...</td>\n",
       "      <td>2015-02-18 00:00:00</td>\n",
       "      <td>Sustainable Progress</td>\n",
       "      <td></td>\n",
       "      <td>https://www.banktrack.org/download/sustainable...</td>\n",
       "      <td>Sustainable Progress Citi's five year sustaina...</td>\n",
       "      <td>pub</td>\n",
       "      <td>50</td>\n",
       "      <td>https://www.banktrack.org/download/sustainable...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                tag  \\\n",
       "109  10979                                            16af3ec   \n",
       "129  11279                                       forestry_pdf   \n",
       "156  11677  sustainable_progress_citi_s_five_year_sustaina...   \n",
       "\n",
       "                    date                          title link_external  \\\n",
       "109  2014-01-01 00:00:00  Sector briefs - Thermal power                 \n",
       "129  2013-12-31 00:00:00  Sustainable Forestry Standard                 \n",
       "156  2015-02-18 00:00:00           Sustainable Progress                 \n",
       "\n",
       "                                         link_internal  \\\n",
       "109         https://www.banktrack.org/download/16af3ec   \n",
       "129    https://www.banktrack.org/download/forestry_pdf   \n",
       "156  https://www.banktrack.org/download/sustainable...   \n",
       "\n",
       "                                                  file pubpriv bp_ids  \\\n",
       "109             1160856_sector_brief_thermal_power.pdf     pub     50   \n",
       "129                                       forestry.pdf     pub     50   \n",
       "156  Sustainable Progress Citi's five year sustaina...     pub     50   \n",
       "\n",
       "                                                  link  \n",
       "109         https://www.banktrack.org/download/16af3ec  \n",
       "129    https://www.banktrack.org/download/forestry_pdf  \n",
       "156  https://www.banktrack.org/download/sustainable...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a dataframe of banktrak links for the chosen bank\n",
    "bt_data_processor = BankTrackDataProcessor()\n",
    "\n",
    "bank_id = bt_data_processor.get_bank_id()\n",
    "\n",
    "df_banktrack_links = bt_data_processor.get_bank_links()\n",
    "df_banktrack_links = df_banktrack_links.query(\"bp_ids == @bank_id\")\n",
    "df_banktrack_links.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PDF metadata for 12 PDFs\n",
      "can't read https://www.citigroup.com/citi/sustainability/policies.htm\n",
      "1 missing values in sample text field. Dropping these rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"Collecting PDF metadata for {len(df_banktrack_links)} PDFs\")\n",
    "df_banktrack_links = garnish_with_pdf_meta(df_banktrack_links, url_field='link')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape HTMLs and PDFs from bank's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with https://www.citigroup.com/citi/sustainability/policies.htm\n",
      "Fetching https://www.citigroup.com/citi/news/2020/200520b.htm url\n",
      "Visited 116 out of 207 unique links\n",
      "Fetching https://www.citigroup.com/mailto:shareholder@computershare.com url\n",
      "Visited 315 out of 428 unique links\n",
      "Fetching https://www.citigroup.com/city_administration/cs_unitedstates.htm url\n",
      "Visited 462 out of 547 unique links\n",
      "Fetching https://www.citigroup.com/citi/#modal/citi-turning-conversations-into-creative-solutions url\n",
      "Visited 562 out of 613 unique links\n",
      "Fetching https://www.citigroup.com/citi/news/2020/200917a.htm url\n",
      "Visited 624 out of 625 unique links\n",
      "Visited all URLs. Stopping the scraper.\n"
     ]
    }
   ],
   "source": [
    "# this dict will store the scraped pdf \n",
    "dct_html_to_links = defaultdict(lambda: defaultdict(lambda: set()))\n",
    "\n",
    "# set start values\n",
    "start_url = list(START_URLS)[0]\n",
    "response = requests.get(start_url)\n",
    "print(f\"Starting with {start_url}\")\n",
    "\n",
    "unique_urls = START_URLS\n",
    "visited_urls = set()\n",
    "counter = 0\n",
    "\n",
    "while len(unique_urls) > len(visited_urls):\n",
    "        \n",
    "    counter += 1\n",
    "    if (counter % 100) == 0:\n",
    "        print(f\"Fetching {response.url} url\")\n",
    "        print(f\"Visited {len(visited_urls)} out of \"\n",
    "              f\"{len(unique_urls)} unique links\")\n",
    "        \n",
    "        if len(unvisited_urls) <= 1:\n",
    "            print(\"Visited all URLs. Stopping the scraper.\")\n",
    "            break\n",
    "    \n",
    "    # get soup from response\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    except TypeError as e:\n",
    "        pass\n",
    "    \n",
    "    # loop through all a tags, extracting links \n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        try:\n",
    "            url = link[\"href\"]\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # pre-pend bank domain to a link if it's not on the banks \n",
    "        # website. This is a hacky way to only stay on the \n",
    "        # bank's website. Can be optimised further\n",
    "        if bank_domain not in url:\n",
    "            absolute_url = BASE_URL + url\n",
    "        else:\n",
    "            absolute_url = url\n",
    "        \n",
    "        # add collected link\n",
    "        unique_urls.add(absolute_url)\n",
    "        \n",
    "        # if link is a pdf AND matches at least one keyword, we store it.\n",
    "        # The link is stored under with the html page as the key\n",
    "        if ('.pdf' in absolute_url) & (get_match_score(absolute_url) > 0):\n",
    "            dct_html_to_links[response.url]['scraped_url'].add(absolute_url)\n",
    "    \n",
    "    # get the urls that we still didn't visit\n",
    "    unvisited_urls = (unique_urls - visited_urls)\n",
    "    \n",
    "    # sort unvisited urls, such that we prioritise those that \n",
    "    # match most keywords\n",
    "    unvisited_urls = sorted(list(unvisited_urls), \n",
    "                            key=get_match_score, reverse=False)\n",
    "    \n",
    "    # define the next url to visit\n",
    "    unvisited_url = unvisited_urls.pop()\n",
    "    \n",
    "    for _ in range(len(unvisited_urls)):\n",
    "        \n",
    "        # while the next url to visit is a pdf, zip or xlsx, then we don't follow it\n",
    "        # and choose another url.\n",
    "        # it can slow things down a lot to scan through a large PDF.\n",
    "        if ('.pdf' in unvisited_url) or unvisited_url.endswith('zip') or \\\n",
    "        unvisited_url.endswith('PDF') or unvisited_url.endswith('xlsx') or \\\n",
    "        unvisited_url.endswith('jpg'):\n",
    "            \n",
    "            visited_urls.add(unvisited_url)\n",
    "            unvisited_url = unvisited_urls.pop()\n",
    "            \n",
    "        else:\n",
    "            # if next url is an html, we fetch response\n",
    "            # and add it to the set of visited urls\n",
    "            try:\n",
    "                response = requests.get(unvisited_url)\n",
    "            except MissingSchema:\n",
    "                visited_urls.add(unvisited_url)\n",
    "                unvisited_url = 'https:' + unvisited_url\n",
    "                response = requests.get(unvisited_url)\n",
    "                unique_urls.add(unvisited_url)\n",
    "                \n",
    "            visited_urls.add(unvisited_url)\n",
    "            break\n",
    "\n",
    "    if len(visited_urls) > 2000:\n",
    "        print(\"Visited over 3000 URLs. Stopping the scraper.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process scraped links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PDF metadata for 19 PDFs\n",
      "can't read https://www.citigroup.com/download/2016/2016-Brazil-Sustainability-Report.pdf\n",
      "can't read https://www.citigroup.com/data/CitiGreenBondSPO-SustainalyticsOpinion.pdf\n",
      "can't read http://www.citigroup.com/citi/environment/data/Corporate_Sustainability_Strategy.pdf\n",
      "3 missing values in sample text field. Dropping these rows\n"
     ]
    }
   ],
   "source": [
    "df_scraped_links = process_scraped_links(dct_html_to_links)\n",
    "df_scraped_links = collect_parent_htmls_per_file(df_scraped_links)\n",
    "\n",
    "print(f\"Collecting PDF metadata for {len(df_scraped_links)} PDFs\")\n",
    "df_scraped_links = garnish_with_pdf_meta(df_scraped_links, url_field='scraped_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge scraped links to banktrack links using pdf text\n",
    "df_banktrack_links = df_banktrack_links.drop_duplicates('sample_text')\n",
    "df_scraped_links = df_scraped_links.drop_duplicates('sample_text')\n",
    "\n",
    "df_all_links = df_banktrack_links.merge(df_scraped_links, on='sample_text', \n",
    "                                        how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check matches and other potentially relevant links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_banktrack_links = len(df_banktrack_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found parent htmls for 3 out of 11 BankTrack stored docs\n",
      "\n",
      "These are: ['https://www.citigroup.com//citi/investor/data/codeconduct_en.pdf', 'https://www.citigroup.com//citi/citizen/data/citi_statement_on_human_rights.pdf', 'https://www.citigroup.com//citi/sustainability/data/Environmental-and-Social-Policy-Framework.pdf']\n",
      "\n",
      "Found the following parent HTMLs for these PDFs:\n",
      "https://www.citigroup.com//citi/fixedincome/green_bonds.htm\n",
      "https://www.citigroup.com//citi/sustainability/climaterisk.htm\n",
      "https://www.citigroup.com/citi/sustainability/policies.htm\n",
      "https://www.citigroup.com//citi/sustainability/lowcarbon.htm\n",
      "https://www.citigroup.com//citi/sustainability/operations.htm\n",
      "https://www.citigroup.com//citi/sustainability/\n"
     ]
    }
   ],
   "source": [
    "df_matches = df_all_links.query(\"_merge == 'both'\")\n",
    "print(f\"Found parent htmls for {len(df_matches)} out of {n_banktrack_links} BankTrack stored docs\\n\")\n",
    "print(f\"These are: {df_matches['scraped_url'].to_list()}\\n\")\n",
    "\n",
    "\n",
    "match_parent_htmls = set(df_matches['parent_htmls'].explode())\n",
    "print(\"Found the following parent HTMLs for these PDFs:\")\n",
    "for phtml in match_parent_htmls:\n",
    "    print(phtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check PDFs only found on Banktrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not find parent htmls for 8 out of 11 BankTrack stored docs\n",
      "\n",
      "https://www.banktrack.org/download/16af3ec\n",
      "https://www.banktrack.org/download/forestry_pdf\n",
      "https://www.banktrack.org/download/sustainable_progress_citi_s_five_year_sustainability_strategy_pdf\n",
      "https://www.banktrack.org/download/citi_antibribery_program\n",
      "https://www.banktrack.org/download/uk_modern_slavery_act_statement\n",
      "https://www.banktrack.org/download/sector_briefs\n",
      "https://www.banktrack.org/download/code_of_ethics_for_financial_professionals\n",
      "https://www.banktrack.org/download/modern_slavery_statement_7\n",
      "\n",
      "NOTE it might be that for some of these PDFs simply the newer version of the doc was scraped. Hence simply the outdated PDF versions stored by BankTrack are not stored by bank anymore\n"
     ]
    }
   ],
   "source": [
    "df_bt_only = df_all_links.query(\"_merge == 'left_only'\")\n",
    "print(f\"Did not find parent htmls for {len(df_bt_only)} \"\n",
    "      f\"out of {len(df_bt_only) + len(df_matches)} BankTrack stored docs\\n\")\n",
    "\n",
    "for pdf in df_bt_only['link']:\n",
    "    print(pdf)\n",
    "\n",
    "print(\"\\nNOTE it might be that for some of these PDFs simply the newer version of the doc was scraped. \"\n",
    "      \"Hence simply the outdated PDF versions stored by BankTrack are not stored by bank anymore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check potentially interesting PDFs and HTMLs not found on BankTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following scraped PDFs were not stored by BankTrack, but match keywords of interest and may be good to track.\n",
      "Sorted from most relevant to least relevant.\n",
      "\n",
      "https://www.citigroup.com//citi/sustainability/data/Impact-Accounting-Methodology.pdf\n",
      "https://www.citigroup.com//citi/sustainability/data/finance-for-a-climate-resilient-future.pdf\n",
      "https://www.citigroup.com//citi/about/esg/download/2019/Executive-Summary-2019.pdf\n",
      "https://www.citigroup.com//citi/investor/data/uk_modern_slavery_statement_2019.pdf\n",
      "https://www.citigroup.com//citi/sustainability/data/Financial-Accounting-Methodology.pdf\n",
      "https://www.citigroup.com//citi/foundation/data/2020-Pathways-Impact-Report.pdf\n",
      "https://www.citigroup.com//citi/about/esg/download/2019/Global-ESG-Report-2019.pdf\n",
      "https://www.citigroup.com/citi/fixedincome/data/opinion_by_sustainalytics.pdf\n"
     ]
    }
   ],
   "source": [
    "df_scraped_only = df_all_links.query(\"_merge == 'right_only'\")\n",
    "scraped_pdfs = set(df_scraped_only['scraped_url'])\n",
    "\n",
    "pdfs_relevancy_mask = np.array(list(map(get_match_score, scraped_pdfs))).nonzero()\n",
    "scraped_pdfs = np.array(list(scraped_pdfs))[pdfs_relevancy_mask]\n",
    "\n",
    "scraped_pdfs = sorted(scraped_pdfs, \n",
    "                      key=get_match_score, reverse=True)\n",
    "\n",
    "print(f\"The following scraped PDFs were not stored by BankTrack, but match \"\n",
    "      f\"keywords of interest and may be good to track.\\n\"\n",
    "      f\"Sorted from most relevant to least relevant.\\n\")\n",
    "for pdf in scraped_pdfs:\n",
    "    print(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following scraped HTMLS were not found when checking parent pages for PDFs stored by BankTrack, but match keywords of interest and may be good to track.\n",
      "Sorted from most relevant to least relevant.\n",
      "\n",
      "https://www.citigroup.com//citi/sustainability/evolution.htm#Equator\n",
      "https://www.citigroup.com//citi/sustainability/evolution.htm#green\n",
      "https://www.citigroup.com/citi/about/citizenship/citi-impact-fund.html\n"
     ]
    }
   ],
   "source": [
    "htmls_of_scraped_pdfs = \\\n",
    "    set(df_all_links[df_all_links['scraped_url'].isin(scraped_pdfs)]['parent_htmls'].explode())\n",
    "\n",
    "scraped_htmls = set(df_scraped_only['parent_htmls'].explode())\n",
    "scraped_htmls = scraped_htmls - match_parent_htmls\n",
    "\n",
    "scraped_htmls = sorted(scraped_htmls, \n",
    "                       key=get_match_score, reverse=True)\n",
    "\n",
    "scraped_htmls = htmls_of_scraped_pdfs.union(scraped_htmls)\n",
    "\n",
    "htmls_relevancy_mask = np.array(list(map(get_match_score, scraped_htmls))).nonzero()\n",
    "scraped_htmls = np.array(list(scraped_htmls))[htmls_relevancy_mask]\n",
    "\n",
    "scraped_htmls = set([html for html in scraped_htmls if 'news' not in html])\n",
    "\n",
    "print(f\"The following scraped HTMLS were not found when checking parent pages for PDFs stored by BankTrack, but match keywords of \"\n",
    "      f\"interest and may be good to track.\\n\"\n",
    "      f\"Sorted from most relevant to least relevant.\\n\")\n",
    "for phtml in scraped_htmls - match_parent_htmls:\n",
    "    print(phtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "1. By a large mergin, the most links on BankTrack are stored internally and don't have an external link right now\n",
    "2. Some pdfs can't be read. One reason is because sometimes the href is relative. Currently the script just prepends the base url, but this doesn't always work. We need to extract the url of the current page and prepend this.\n",
    "3. Some PDFs are locked for copy. The method that is used currently fails to convert it to text because of this. So we can't match these PDFs on text.\n",
    "4. There are some relevant PDFs and HTMLs returned by the scraper, but not all are relevant. Can be stricter with keywords to filter these out. Especially true for banks like Barclays, that return a lot of relevant links. Links for parent HTMLs of PDFs that are stored on BankTrack, are definitely relevant. Then there are also HTMLs and PDFs that were not found on BankTrack, but may be relevant. Some of these PDFs are for docs that bank track does store, but the updated version, e.g. BankTrack might have Human-Rights-2020 but we scrape Human-Rights-2021 from bank's website\n",
    "5. Sometimes the BankTrack stored PDFs seemingly cannot be found on the banks website. Some of these cases could be attributed that a new version is available, as stated in point 4. Perhaps some PDFs are only accessible through a google site search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
