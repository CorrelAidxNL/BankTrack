#+TITLE: Changedetection.io
#+AUTHOR: Tim Loderhose | Machine Learning Programs
#+EMAIL: tim.loderhose@mlprograms.com
#+DATE: Tuesday, 11 January 2022
#+STARTUP: showall
#+PROPERTY: header-args :exports both :session cd :kernel banktrack :cache no
:PROPERTIES:
#+OPTIONS: ^:nil
#+LATEX_COMPILER: xelatex
#+LATEX_CLASS: mlpdoc
#+LATEX_CLASS_OPTIONS: [logo, color, author]
#+LATEX_HEADER: \headertitle{Change to your desired header}
#+LATEX_HEADER: \insertauthor
#+LATEX_HEADER: \versiondoc{v0.1 - November 19, 2020}
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \setminted{bgcolor=WhiteSmoke}
:END:

* Imports and Environment Variables
:PROPERTIES:
:visibility: folded
:END:

#+name: imports
#+begin_src python :results silent
from pathlib import Path
import json
#+end_src

#+name: env
#+begin_src python :results silent
data_dir = Path("../data")
bank_urls = data_dir / "bank_urls.json"
#+end_src

* Introduction
[[changedetection.io][changedetection.io]] is a tool that on the surface does exactly what we want:
- hosted web interface
- add websites
  - differ that runs in preset intervals

* Put initial list of URLs on the watchlist

I checked out the ~bank_urls.json~ from Luka's =spike-scraping= branch, and will import
these as .txt into changedetection.
#+begin_src python
with open(bank_urls, "r") as f:
    bu = json.load(f)

html_links = [x for bank in bu for x in bank["urls"]]
with open(data_dir / "urls.txt", "w") as f:
    f.writelines(map(lambda x: x + "\n", html_links))
#+end_src

#+RESULTS:

This worked like a charm, and these URLs are now tracked in changedetection.

* Changes in 5 days:
Number of pages in parentheses.

None of them are interesting. Changes that show up across all websites should be
ignored.
- this could possibly achieved by checking diffs, and ignoring changes if they occur on
  all websites. Possibly hide all but one of the changes?

** BNPParibas (4)
- No. jobs available on all pages
- Stock price and time
  - will always show up
** BOCHK (1)
- Some news, * more links
** USBank (2)
- navigation - new sections
** Unicreditgroup (1)
- whitespace
** Societe Generale (2)
- stock price
** Shinhan group (3)
- stock price
** RaboBank BR (1)
- whitespace
** Nordea (1)
- "In Focus" showcasing articles on sustainability (links)
** DB (12)
- News
- Link to ESG conference (most pages)
** Credit Agricole (2)
- stock price
** Commerzbank (5)
- date
** China Everbright Environment Group
- news

* Changedetection.io code

Quality not great, or written in a style I don't like. Ie.

** Flask webapp at [[file:../../changedetection.io/changedetectionio/__init__.py::def user_loader(email):]]
Run through an Eventlet WSGI server (https://eventlet.net/)

*** [[file:../../changedetection.io/changedetectionio/__init__.py::def do_something_whenever_a_request_comes_in():]]
Not much confidence in the code here...

*** Pages
simple enough

*** [[file:../../changedetection.io/changedetectionio/__init__.py::def check_for_new_version():]]
Sends version, UID & watch count to homepage every use.
- returns check for new version

*** [[file:../../changedetection.io/changedetectionio/__init__.py::def import_page():][Import]]
1. Open page with GET in =/import=
   1. renders template of import.html, Jinja2
   2. Contains textarea which will be submitted on submit
2. On submission, goes through URLs:
   1. Strip
   2. Validate
   3. ~datastore.add_watch(url, tag="")~
   4. Counts good and skipped URLs and redirects to index

Simple to add tags.

** Datastore - [[file:../../changedetection.io/changedetectionio/store.py::class ChangeDetectionStore:]]
single JSON file storing all watches. Almost 6k lines ~6 days after start, running the
tool twice.

-
  - This could get too large. If we have 250kB after a week, this would be a 12MB text
    file after a year - if the number of websites indexed don't change, and if only
    checked twice per week.

+
  - If this runs as a live webservice, there is no issue with loading the JSON store, it's
    probably append-only, which could happen once a week
  - Can scrub history

*** Deleting a watch also deletes its history
[[file:../../changedetection.io/changedetectionio/store.py::def delete(self, uuid):]]

*** [[file:../../changedetection.io/changedetectionio/store.py::def scrub_watch(self, uuid, limit_timestamp = False):]]
Keep watch, but delete its data to keep JSON store lean.
- probably have to do every once in a while - after backing up though!

** How to interact with the repo?
- project moves fast
- many issues/PRs
